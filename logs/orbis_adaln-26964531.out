=== Starting SLURM Job on lmbhiwi_gpu-rtx2080 ===
Node: dagobert
Job ID: 26964531
Python = /work/dlclarge2/alidemaa-text-control-orbis/miniconda3/envs/orbis_env/bin/python
Torch CUDA available?:
True
Sat Dec  6 01:15:13 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:1A:00.0 Off |                  N/A |
| 23%   35C    P8             17W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.
Using device: cuda
Loading FM config from: /work/dlclarge2/alidemaa-text-control-orbis/orbis/finetuning/stage2_baseline_covla_bev.yaml
Train dataset target from YAML: data.covla_dataset.CoVLAOrbisMultiFrame
YAML data params (train / CoVLA):
  size            = [192, 336]
  num_frames      = 6
  stored_rate     = 20
  target_rate     = 5
  scale_min/max   = 0.75, 1.0
  captions_dir    = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_captions
  videos_dir      = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_100_videos
Text encoder initialized & frozen.

Loading tokenizer from stage1 config...
VQLPIPSWithDiscriminator initialized with hinge loss.
Tokenizer ready.

Loading STDiT (world model)...
STDiT pretrained weights loaded (strict=False).
Trainable parameters: 65,359,104
[CKPT] Found existing checkpoint: /work/dlclarge2/alidemaa-text-control-orbis/orbis/finetuning/checkpoints/adaln_resume.ckpt
[CKPT] Resuming from epoch=20

Starting AdaLN fine-tuning...

[TRAIN] Starting epoch 21/30
Epoch 20 finished — avg_loss=0.9993
[TRAIN] Starting epoch 22/30
Epoch 21 finished — avg_loss=1.0000
[TRAIN] Starting epoch 23/30
Epoch 22 finished — avg_loss=1.0013
[TRAIN] Starting epoch 24/30
Epoch 23 finished — avg_loss=0.9985
[TRAIN] Starting epoch 25/30
Epoch 24 finished — avg_loss=1.0017
[TRAIN] Starting epoch 26/30
Epoch 25 finished — avg_loss=0.9995
[TRAIN] Starting epoch 27/30
Epoch 26 finished — avg_loss=0.9988
[TRAIN] Starting epoch 28/30
Epoch 27 finished — avg_loss=1.0013
[TRAIN] Starting epoch 29/30
Epoch 28 finished — avg_loss=0.9991
[TRAIN] Starting epoch 30/30
Epoch 29 finished — avg_loss=1.0006
[CKPT] Saved checkpoint at epoch=29, step=199

Done! AdaLN fine-tune saved → /work/dlclarge2/alidemaa-text-control-orbis/orbis/finetuning/finetuned_orbis_AdaLN.ckpt
=== Job Finished ===
