=== Starting SLURM Job on lmbhiwi_gpu-rtx2080 ===
Node: dagobert
Job ID: 26976113
Python = /work/dlclarge2/alidemaa-text-control-orbis/miniconda3/envs/orbis_env/bin/python
Torch CUDA available?:
True
Mon Dec 15 04:30:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 2080 Ti     On  |   00000000:1A:00.0 Off |                  N/A |
| 22%   28C    P8             11W /  250W |       1MiB /  11264MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
Using device: cuda
Loading FM config from: /work/dlclarge2/alidemaa-text-control-orbis/orbis/finetuning/fm_finetune_covla_modelif.yaml
[FM CONFIG] Overriding tokenizer_config with local paths:
  folder    = /work/dlclarge2/alidemaa-text-control-orbis/orbis/logs_tk/tokenizer_192x336
  ckpt_path = checkpoints/epoch-26_rfid_8_9.ckpt
Train dataset target from YAML: data.covla_dataset.CoVLAOrbisMultiFrame
Validation dataset target from YAML: data.covla_dataset.CoVLAOrbisMultiFrame
YAML data params (train / CoVLA):
  size            = [192, 336]
  num_frames      = 6
  stored_rate     = 20
  target_rate     = 5
  captions_dir    = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_captions
  videos_dir      = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_100_videos
YAML data params (val / CoVLA):
  size            = [192, 336]
  num_frames      = 6
  stored_rate     = 20
  target_rate     = 5
  captions_dir    = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_captions
  videos_dir      = /work/dlclarge2/alidemaa-text-control-orbis/orbis/data/covla_20_validation

[FM CONFIG] Using Stage2 FM checkpoint:
  ORBIT_CKPT = /work/dlclarge2/alidemaa-text-control-orbis/orbis/logs_wm/orbis_288x512/checkpoints/last.ckpt

Text encoder initialized & frozen.

Loading FM world model (Model from fm_model.py)...
[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.
[fm_model] WARNING: Encoder does not support 'use_pretrained_weights'. Dropping this flag (it is false / handled by checkpoint).
VQLPIPSWithDiscriminator initialized with hinge loss.
Loaded world model ckpt. Missing keys: 4, unexpected: 0
World model ready (STDiT backbone, tokenizer, noise schedule, sampling).
[CKPT] No existing checkpoint found → starting from scratch
[RUN] Saving samples to: finetuning/samples/26976113
[RUN] TensorBoard logs in: finetuning/runs/26976113

Starting AdaLN fine-tuning using fm_model noise & encoder...

[SAMPLE] Saved finetuning/samples/26976113/sample_before_train.png
[TRAIN] Starting epoch 1/50
[GRAD][text_mlp] text_mlp.1.weight | 0.000e+00
[GRAD][text_mlp] text_mlp.1.weight | 1.216e-02
[TRAIN] Epoch 1 finished — avg_loss=0.3160
[VAL] Epoch 1 — avg_val_loss=0.2454
[CKPT] Saved checkpoint at epoch=0, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch1.png
[TRAIN] Starting epoch 2/50
[GRAD][text_mlp] text_mlp.1.weight | 1.366e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.231e-02
[TRAIN] Epoch 2 finished — avg_loss=0.3361
[VAL] Epoch 2 — avg_val_loss=0.2766
[CKPT] Saved checkpoint at epoch=1, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch2.png
[TRAIN] Starting epoch 3/50
[GRAD][text_mlp] text_mlp.1.weight | 1.768e-02
[GRAD][text_mlp] text_mlp.1.weight | 6.445e-03
[TRAIN] Epoch 3 finished — avg_loss=0.2888
[VAL] Epoch 3 — avg_val_loss=0.2813
[CKPT] Saved checkpoint at epoch=2, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch3.png
[TRAIN] Starting epoch 4/50
[GRAD][text_mlp] text_mlp.1.weight | 1.201e-02
[GRAD][text_mlp] text_mlp.1.weight | 8.565e-03
[TRAIN] Epoch 4 finished — avg_loss=0.2973
[VAL] Epoch 4 — avg_val_loss=0.2221
[CKPT] Saved checkpoint at epoch=3, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch4.png
[TRAIN] Starting epoch 5/50
[GRAD][text_mlp] text_mlp.1.weight | 9.859e-03
[GRAD][text_mlp] text_mlp.1.weight | 9.269e-03
[TRAIN] Epoch 5 finished — avg_loss=0.2969
[VAL] Epoch 5 — avg_val_loss=0.2763
[CKPT] Saved checkpoint at epoch=4, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch5.png
[TRAIN] Starting epoch 6/50
[GRAD][text_mlp] text_mlp.1.weight | 1.985e-01
[GRAD][text_mlp] text_mlp.1.weight | 1.892e-02
[TRAIN] Epoch 6 finished — avg_loss=0.4764
[VAL] Epoch 6 — avg_val_loss=0.2148
[CKPT] Saved checkpoint at epoch=5, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch6.png
[TRAIN] Starting epoch 7/50
[GRAD][text_mlp] text_mlp.1.weight | 2.772e-02
[GRAD][text_mlp] text_mlp.1.weight | 6.802e-02
[TRAIN] Epoch 7 finished — avg_loss=0.4703
[VAL] Epoch 7 — avg_val_loss=0.2188
[CKPT] Saved checkpoint at epoch=6, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch7.png
[TRAIN] Starting epoch 8/50
[GRAD][text_mlp] text_mlp.1.weight | 4.803e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.671e-03
[TRAIN] Epoch 8 finished — avg_loss=0.4542
[VAL] Epoch 8 — avg_val_loss=0.2774
[CKPT] Saved checkpoint at epoch=7, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch8.png
[TRAIN] Starting epoch 9/50
[GRAD][text_mlp] text_mlp.1.weight | 1.923e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.775e-02
[TRAIN] Epoch 9 finished — avg_loss=0.4671
[VAL] Epoch 9 — avg_val_loss=0.2308
[CKPT] Saved checkpoint at epoch=8, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch9.png
[TRAIN] Starting epoch 10/50
[GRAD][text_mlp] text_mlp.1.weight | 1.204e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.957e-02
[TRAIN] Epoch 10 finished — avg_loss=0.4494
[VAL] Epoch 10 — avg_val_loss=0.2890
[CKPT] Saved checkpoint at epoch=9, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch10.png
[TRAIN] Starting epoch 11/50
[GRAD][text_mlp] text_mlp.1.weight | 1.141e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.794e-02
[TRAIN] Epoch 11 finished — avg_loss=0.4724
[VAL] Epoch 11 — avg_val_loss=0.2202
[CKPT] Saved checkpoint at epoch=10, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch11.png
[TRAIN] Starting epoch 12/50
[GRAD][text_mlp] text_mlp.1.weight | 1.656e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.599e-02
[TRAIN] Epoch 12 finished — avg_loss=0.4092
[VAL] Epoch 12 — avg_val_loss=0.2529
[CKPT] Saved checkpoint at epoch=11, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch12.png
[TRAIN] Starting epoch 13/50
[GRAD][text_mlp] text_mlp.1.weight | 2.858e-02
[GRAD][text_mlp] text_mlp.1.weight | 5.756e-02
[TRAIN] Epoch 13 finished — avg_loss=0.4753
[VAL] Epoch 13 — avg_val_loss=0.2897
[CKPT] Saved checkpoint at epoch=12, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch13.png
[TRAIN] Starting epoch 14/50
[GRAD][text_mlp] text_mlp.1.weight | 1.654e-01
[GRAD][text_mlp] text_mlp.1.weight | 3.902e-02
[TRAIN] Epoch 14 finished — avg_loss=0.4622
[VAL] Epoch 14 — avg_val_loss=0.2283
[CKPT] Saved checkpoint at epoch=13, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch14.png
[TRAIN] Starting epoch 15/50
[GRAD][text_mlp] text_mlp.1.weight | 3.733e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.978e-02
[TRAIN] Epoch 15 finished — avg_loss=0.4399
[VAL] Epoch 15 — avg_val_loss=0.2148
[CKPT] Saved checkpoint at epoch=14, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch15.png
[TRAIN] Starting epoch 16/50
[GRAD][text_mlp] text_mlp.1.weight | 3.353e-03
[GRAD][text_mlp] text_mlp.1.weight | 1.185e-01
[TRAIN] Epoch 16 finished — avg_loss=0.4581
[VAL] Epoch 16 — avg_val_loss=0.2718
[CKPT] Saved checkpoint at epoch=15, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch16.png
[TRAIN] Starting epoch 17/50
[GRAD][text_mlp] text_mlp.1.weight | 2.208e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.466e-02
[TRAIN] Epoch 17 finished — avg_loss=0.4673
[VAL] Epoch 17 — avg_val_loss=0.2594
[CKPT] Saved checkpoint at epoch=16, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch17.png
[TRAIN] Starting epoch 18/50
[GRAD][text_mlp] text_mlp.1.weight | 4.386e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.431e-02
[TRAIN] Epoch 18 finished — avg_loss=0.4455
[VAL] Epoch 18 — avg_val_loss=0.2594
[CKPT] Saved checkpoint at epoch=17, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch18.png
[TRAIN] Starting epoch 19/50
[GRAD][text_mlp] text_mlp.1.weight | 1.702e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.172e-02
[TRAIN] Epoch 19 finished — avg_loss=0.4586
[VAL] Epoch 19 — avg_val_loss=0.2752
[CKPT] Saved checkpoint at epoch=18, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch19.png
[TRAIN] Starting epoch 20/50
[GRAD][text_mlp] text_mlp.1.weight | 2.428e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.769e-02
[TRAIN] Epoch 20 finished — avg_loss=0.4898
[VAL] Epoch 20 — avg_val_loss=0.2360
[CKPT] Saved checkpoint at epoch=19, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch20.png
[TRAIN] Starting epoch 21/50
[GRAD][text_mlp] text_mlp.1.weight | 2.218e-02
[GRAD][text_mlp] text_mlp.1.weight | 8.558e-03
[TRAIN] Epoch 21 finished — avg_loss=0.4728
[VAL] Epoch 21 — avg_val_loss=0.2185
[CKPT] Saved checkpoint at epoch=20, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch21.png
[TRAIN] Starting epoch 22/50
[GRAD][text_mlp] text_mlp.1.weight | 1.261e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.382e-02
[TRAIN] Epoch 22 finished — avg_loss=0.4842
[VAL] Epoch 22 — avg_val_loss=0.2631
[CKPT] Saved checkpoint at epoch=21, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch22.png
[TRAIN] Starting epoch 23/50
[GRAD][text_mlp] text_mlp.1.weight | 4.563e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.659e-02
[TRAIN] Epoch 23 finished — avg_loss=0.4662
[VAL] Epoch 23 — avg_val_loss=0.2369
[CKPT] Saved checkpoint at epoch=22, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch23.png
[TRAIN] Starting epoch 24/50
[GRAD][text_mlp] text_mlp.1.weight | 5.770e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.853e-02
[TRAIN] Epoch 24 finished — avg_loss=0.4726
[VAL] Epoch 24 — avg_val_loss=0.2527
[CKPT] Saved checkpoint at epoch=23, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch24.png
[TRAIN] Starting epoch 25/50
[GRAD][text_mlp] text_mlp.1.weight | 8.825e-03
[GRAD][text_mlp] text_mlp.1.weight | 3.312e-02
[TRAIN] Epoch 25 finished — avg_loss=0.4920
[VAL] Epoch 25 — avg_val_loss=0.2011
[CKPT] Saved checkpoint at epoch=24, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch25.png
[TRAIN] Starting epoch 26/50
[GRAD][text_mlp] text_mlp.1.weight | 1.912e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.156e-02
[TRAIN] Epoch 26 finished — avg_loss=0.4734
[VAL] Epoch 26 — avg_val_loss=0.1729
[CKPT] Saved checkpoint at epoch=25, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch26.png
[TRAIN] Starting epoch 27/50
[GRAD][text_mlp] text_mlp.1.weight | 3.542e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.953e-02
[TRAIN] Epoch 27 finished — avg_loss=0.4710
[VAL] Epoch 27 — avg_val_loss=0.2305
[CKPT] Saved checkpoint at epoch=26, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch27.png
[TRAIN] Starting epoch 28/50
[GRAD][text_mlp] text_mlp.1.weight | 2.014e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.442e-02
[TRAIN] Epoch 28 finished — avg_loss=0.4761
[VAL] Epoch 28 — avg_val_loss=0.2903
[CKPT] Saved checkpoint at epoch=27, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch28.png
[TRAIN] Starting epoch 29/50
[GRAD][text_mlp] text_mlp.1.weight | 5.138e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.484e-02
[TRAIN] Epoch 29 finished — avg_loss=0.4822
[VAL] Epoch 29 — avg_val_loss=0.2337
[CKPT] Saved checkpoint at epoch=28, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch29.png
[TRAIN] Starting epoch 30/50
[GRAD][text_mlp] text_mlp.1.weight | 1.768e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.492e-02
[TRAIN] Epoch 30 finished — avg_loss=0.4371
[VAL] Epoch 30 — avg_val_loss=0.2118
[CKPT] Saved checkpoint at epoch=29, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch30.png
[TRAIN] Starting epoch 31/50
[GRAD][text_mlp] text_mlp.1.weight | 3.749e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.048e-01
[TRAIN] Epoch 31 finished — avg_loss=0.4271
[VAL] Epoch 31 — avg_val_loss=0.3062
[CKPT] Saved checkpoint at epoch=30, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch31.png
[TRAIN] Starting epoch 32/50
[GRAD][text_mlp] text_mlp.1.weight | 6.255e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.125e-02
[TRAIN] Epoch 32 finished — avg_loss=0.4850
[VAL] Epoch 32 — avg_val_loss=0.2774
[CKPT] Saved checkpoint at epoch=31, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch32.png
[TRAIN] Starting epoch 33/50
[GRAD][text_mlp] text_mlp.1.weight | 3.860e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.217e-02
[TRAIN] Epoch 33 finished — avg_loss=0.4740
[VAL] Epoch 33 — avg_val_loss=0.1904
[CKPT] Saved checkpoint at epoch=32, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch33.png
[TRAIN] Starting epoch 34/50
[GRAD][text_mlp] text_mlp.1.weight | 3.891e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.981e-02
[TRAIN] Epoch 34 finished — avg_loss=0.4959
[VAL] Epoch 34 — avg_val_loss=0.2724
[CKPT] Saved checkpoint at epoch=33, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch34.png
[TRAIN] Starting epoch 35/50
[GRAD][text_mlp] text_mlp.1.weight | 3.417e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.837e-02
[TRAIN] Epoch 35 finished — avg_loss=0.4501
[VAL] Epoch 35 — avg_val_loss=0.2272
[CKPT] Saved checkpoint at epoch=34, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch35.png
[TRAIN] Starting epoch 36/50
[GRAD][text_mlp] text_mlp.1.weight | 3.323e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.427e-02
[TRAIN] Epoch 36 finished — avg_loss=0.4850
[VAL] Epoch 36 — avg_val_loss=0.2949
[CKPT] Saved checkpoint at epoch=35, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch36.png
[TRAIN] Starting epoch 37/50
[GRAD][text_mlp] text_mlp.1.weight | 2.157e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.559e-02
[TRAIN] Epoch 37 finished — avg_loss=0.4451
[VAL] Epoch 37 — avg_val_loss=0.2538
[CKPT] Saved checkpoint at epoch=36, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch37.png
[TRAIN] Starting epoch 38/50
[GRAD][text_mlp] text_mlp.1.weight | 1.567e-02
[GRAD][text_mlp] text_mlp.1.weight | 6.068e-02
[TRAIN] Epoch 38 finished — avg_loss=0.4964
[VAL] Epoch 38 — avg_val_loss=0.2529
[CKPT] Saved checkpoint at epoch=37, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch38.png
[TRAIN] Starting epoch 39/50
[GRAD][text_mlp] text_mlp.1.weight | 4.027e-02
[GRAD][text_mlp] text_mlp.1.weight | 3.784e-02
[TRAIN] Epoch 39 finished — avg_loss=0.4610
[VAL] Epoch 39 — avg_val_loss=0.1832
[CKPT] Saved checkpoint at epoch=38, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch39.png
[TRAIN] Starting epoch 40/50
[GRAD][text_mlp] text_mlp.1.weight | 2.585e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.606e-02
[TRAIN] Epoch 40 finished — avg_loss=0.4658
[VAL] Epoch 40 — avg_val_loss=0.2421
[CKPT] Saved checkpoint at epoch=39, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch40.png
[TRAIN] Starting epoch 41/50
[GRAD][text_mlp] text_mlp.1.weight | 3.875e-02
[GRAD][text_mlp] text_mlp.1.weight | 4.775e-02
[TRAIN] Epoch 41 finished — avg_loss=0.4629
[VAL] Epoch 41 — avg_val_loss=0.2474
[CKPT] Saved checkpoint at epoch=40, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch41.png
[TRAIN] Starting epoch 42/50
[GRAD][text_mlp] text_mlp.1.weight | 3.228e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.724e-02
[TRAIN] Epoch 42 finished — avg_loss=0.4769
[VAL] Epoch 42 — avg_val_loss=0.2351
[CKPT] Saved checkpoint at epoch=41, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch42.png
[TRAIN] Starting epoch 43/50
[GRAD][text_mlp] text_mlp.1.weight | 7.436e-02
[GRAD][text_mlp] text_mlp.1.weight | 5.619e-02
[TRAIN] Epoch 43 finished — avg_loss=0.5028
[VAL] Epoch 43 — avg_val_loss=0.2513
[CKPT] Saved checkpoint at epoch=42, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch43.png
[TRAIN] Starting epoch 44/50
[GRAD][text_mlp] text_mlp.1.weight | 5.549e-02
[GRAD][text_mlp] text_mlp.1.weight | 4.430e-02
[TRAIN] Epoch 44 finished — avg_loss=0.4710
[VAL] Epoch 44 — avg_val_loss=0.2523
[CKPT] Saved checkpoint at epoch=43, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch44.png
[TRAIN] Starting epoch 45/50
[GRAD][text_mlp] text_mlp.1.weight | 6.769e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.746e-02
[TRAIN] Epoch 45 finished — avg_loss=0.4400
[VAL] Epoch 45 — avg_val_loss=0.2021
[CKPT] Saved checkpoint at epoch=44, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch45.png
[TRAIN] Starting epoch 46/50
[GRAD][text_mlp] text_mlp.1.weight | 1.408e-02
[GRAD][text_mlp] text_mlp.1.weight | 2.384e-02
[TRAIN] Epoch 46 finished — avg_loss=0.4402
[VAL] Epoch 46 — avg_val_loss=0.3362
[CKPT] Saved checkpoint at epoch=45, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch46.png
[TRAIN] Starting epoch 47/50
[GRAD][text_mlp] text_mlp.1.weight | 3.895e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.825e-02
[TRAIN] Epoch 47 finished — avg_loss=0.4545
[VAL] Epoch 47 — avg_val_loss=0.2164
[CKPT] Saved checkpoint at epoch=46, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch47.png
[TRAIN] Starting epoch 48/50
[GRAD][text_mlp] text_mlp.1.weight | 6.542e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.409e-01
[TRAIN] Epoch 48 finished — avg_loss=0.5071
[VAL] Epoch 48 — avg_val_loss=0.2626
[CKPT] Saved checkpoint at epoch=47, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch48.png
[TRAIN] Starting epoch 49/50
[GRAD][text_mlp] text_mlp.1.weight | 2.196e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.418e-02
[TRAIN] Epoch 49 finished — avg_loss=0.4671
[VAL] Epoch 49 — avg_val_loss=0.2891
[CKPT] Saved checkpoint at epoch=48, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch49.png
[TRAIN] Starting epoch 50/50
[GRAD][text_mlp] text_mlp.1.weight | 9.259e-02
[GRAD][text_mlp] text_mlp.1.weight | 1.877e-02
[TRAIN] Epoch 50 finished — avg_loss=0.4959
[VAL] Epoch 50 — avg_val_loss=0.2853
[CKPT] Saved checkpoint at epoch=49, step=199
[SAMPLE] Saved finetuning/samples/26976113/sample_epoch50.png

Done! AdaLN fine-tune saved → /work/dlclarge2/alidemaa-text-control-orbis/orbis/finetuning/third_train.ckpt
[RUN] TensorBoard writer closed.
=== Job Finished ===
